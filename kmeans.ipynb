{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will go over what K-Means Clustering is, and how to implement it with RAPIDS cuML.\n",
    "\n",
    "This Notebook can be run with a free GPU at [app.blazingsql.com](https://bit.ly/intro_ds_notebooks): `git clone https://github.com/Dropout-Analytics/cuml_kmeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Intro to K-Means Clustering with cuML](https://medium.com/dropout-analytics/intro-to-k-means-clustering-with-cuml-b6d617e36456?source=friends_link&sk=ce3b63fd8f41c9bdfbd04fa4f40b2365)\n",
    "\n",
    "K-Means is an easy way to cluster data. It randomly selects K points (centroids) in a given dataset, then computes which of the dataset's instances are closest to each point (making a cluster). \n",
    "\n",
    "For every cluster, the mean of its values (instances) is computed, and this mean becomes that cluster's new point (centroid).\n",
    "\n",
    "Once a cluster's centroid has moved, its distance from the dataset's instances has changed, and instances may be added to or removed from that cluster. The mean is recalculated & replaced until it stops moving or has hit the given number maximum iterations (`max_iter`), whichever comes first.\n",
    "\n",
    "![K-Means visual](https://cdn-images-1.medium.com/max/800/0*AVBuF9liGiN3RaR6.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this intro, we'll be working with [Fischer's Iris dataset](https://wikipedia.org/wiki/Iris_flower_data_set). The set holds 3 species of Iris flower (Iris setosa, Iris virginica and Iris versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.read_csv('https://github.com/gumdropsteve/datasets/raw/master/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris species](https://camo.githubusercontent.com/224f964448da6133df6dcb316c6a0352a5403de6/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3630302f302a585762417a384a53704478736e633864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 features - the length & width of the sepals & petals -  were measured in centimeters from each flower; each species was recorded 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize \n",
    "Let's see what the model is working with. As we already know this dataset's target values (`species`), we can easily visualize the different clusters with Matplotlib. Our K-Means model will not be given this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pandas().plot(kind='scatter', x='petal_width', y='sepal_length', \n",
    "                    c='target', cmap=('rainbow'), sharex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuML KMeans\n",
    "> cuML’s [KMeans](https://docs.rapids.ai/api/cuml/stable/api.html#k-means-clustering) supports the scalable [KMeans++](https://en.wikipedia.org/wiki/K-means%2B%2B) initialization method [(`init='scalable-k-means++'` or `'k-means||'`)]. This method is more stable than randomly selecting K points [(`init='random'`)]. \n",
    ">\n",
    "> The model can take array-like objects, either in host as NumPy arrays or in device (as Numba or cuda_array_interface-compliant), as well as cuDF DataFrames as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, max_iter=300, init='scalable-k-means++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the purpose of K-Means is to determine the data's clusters, we don't need to give the model the `species` or `target` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['species', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.fit()` the model with our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Results\n",
    "We can score our model with cuML's `adjusted_rand_score()` function, which is a [Rand index](https://en.wikipedia.org/wiki/Rand_index) adjusted for chance.\n",
    "\n",
    "The Rand index (RI) computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The raw RI score is then “adjusted for chance” into the Adjusted Rand index (ARI) score using the following scheme:\n",
    "```python\n",
    "ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "```\n",
    "\n",
    "> The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples, exactly 1.0 when the clusterings are identical, and -1 (up to a permutation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import adjusted_rand_score\n",
    "\n",
    "score = adjusted_rand_score(labels_true=df['target'], \n",
    "                            labels_pred=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add our `kmeans.labels_` to a `.copy()` of the original `df` and see how they look.\n",
    "\n",
    "Note: The model was never made aware of the actual clusters, and came up with its own, so comparing the `target` and `predicted` patterns will give you a more accurate understanding than comparing the `predicted` and `actual` values here. _For high scores on this dataset, this usually means the 2s match up and the 1s and 0s are flipped._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df = df.copy()\n",
    "\n",
    "results_df['predicted'] = kmeans.labels_\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the difference in species' respective target values (so the colors will be in a different order) in mind, let's visualize our model's clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.to_pandas().plot(kind='scatter', x='petal_width', y='sepal_length', \n",
    "                    c=kmeans.labels_.to_array(), cmap=('rainbow'), sharex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Determine K?\n",
    "Well, it really helps if you just know how many clusters there are. But if you don't, there are a number of ways to figure out the best value for K.\n",
    "\n",
    "### The 'Elbow' method\n",
    "With the Elbow method, we simply plot the within-cluster sum of squares (RSS) and try to see what looks like an elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_score(nclust):\n",
    "    km = KMeans(n_clusters=nclust)\n",
    "    km.fit(X)\n",
    "    rss = -km.score(X)\n",
    "    return rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [kmean_score(i) for i in range(1, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the 'elbow' point allows us to capitalize on reduction in variation as K increases. You should pick K where the reduction in variation flattens off (comparatively). In our case, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(1, 8), scores)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('RSS')\n",
    "plt.title('RSS versus K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette score\n",
    "The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. \n",
    "\n",
    "For each point $x_i$:\n",
    "\n",
    "- $a(i)$ average dissimilarity of $x_i$ with points in the same cluster\n",
    "- $b(i)$ average dissimilarity of $x_i$ with points in the nearest cluster\n",
    "    \"nearest\" means cluster with the smallest $b(i)$\n",
    "\n",
    "$$\\text{silhouette}(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))} $$\n",
    "\n",
    "Silhouette scores range from -1 to 1. \n",
    "\n",
    "> Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "The silhouette score of a clustering is the average of silhouette score of all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def get_silhouette_score(nclust):\n",
    "    km = KMeans(n_clusters=nclust)\n",
    "    km.fit(X)\n",
    "    sil_avg = silhouette_score(X.to_pandas(), km.labels_.to_pandas())\n",
    "    return sil_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = [get_silhouette_score(i) for i in range(2, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(2, 8), sil_scores)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Learning \n",
    "Here are some resources I recommend to help fill in any gaps and provide a more complete picture.\n",
    "\n",
    "#### **StatQuest: K-means clustering**\n",
    "- Watch on YouTube: [https://youtu.be/4b5d3muPQmA](https://youtu.be/4b5d3muPQmA)\n",
    "- Channel: StatQuest with Josh Starmer ([Subscribe](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw?sub_confirmation=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('4b5d3muPQmA', width=(1280*0.69), height=(720*0.69))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means++:  The Advantages of Careful Seeding\n",
    "- Stanford InfoLab Publication Server: [ilpubs.stanford.edu:8090/778/1/2006-13.pdf](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)\n",
    "- by David Arthur and Sergei Vassilvitskii\n",
    "\n",
    "#### Scalable K-Means++\n",
    "- Very Large Data Bases Endowment Inc.: [vldb.org/pvldb/vol5/p622_bahmanbahmani_vldb2012.pdf](http://vldb.org/pvldb/vol5/p622_bahmanbahmani_vldb2012.pdf)\n",
    "- by Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei Vassilvitskii\n",
    "\n",
    "#### **K-means clustering**\n",
    "Wikipedia: [wikipedia.org/wiki/K-means_clustering](https://wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [**Back to GitHub**](https://github.com/Dropout-Analytics/cuml_kmeans) | [Sponsor this Project](https://github.com/sponsors/gumdropsteve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
